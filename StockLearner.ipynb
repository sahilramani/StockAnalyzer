{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enormous-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "variable-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_analyzer import (csv_loader, data_ext)\n",
    "from stock_analyzer.models import (StockLSTM, StockGRU)\n",
    "\n",
    "raw_data = csv_loader.load_symbol('./data/raw/', 'TSLA', '1min', '2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reflected-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader)\n",
    "def create_dataloader(data, lookback, bs):\n",
    "    x = [data[i:i+lookback, :] for i in range(len(data)-lookback)]\n",
    "    y = [[data[i, 0]] for i in range(lookback, len(data))]\n",
    "    ds = TensorDataset(torch.as_tensor(x).float().to(device), torch.as_tensor(y).float().to(device))\n",
    "    loader = DataLoader(ds, shuffle=True, batch_size=bs)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "least-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "train_scaler = MinMaxScaler()\n",
    "val_scaler = MinMaxScaler()\n",
    "test_scaler = MinMaxScaler()\n",
    "\n",
    "seq_length = 20\n",
    "batch_size = 256\n",
    "\n",
    "train_count, val_count, test_count = data_ext.split_counts(raw_data.shape[0], seq_length, train_per=0.7, val_per=0.2)\n",
    "\n",
    "# 'close' is required and must be on index 0, everything else is optional\n",
    "features = ['close', 'volume', 'open', 'high', 'low', 'adx', 'ema']\n",
    "train_data = train_scaler.fit_transform(raw_data[features].values[:train_count+1])\n",
    "val_data = val_scaler.fit_transform(raw_data[features].values[train_count:train_count+val_count+1])\n",
    "test_data = test_scaler.fit_transform(raw_data[features].values[train_count+val_count:train_count+val_count+test_count+1])\n",
    "\n",
    "train_loader = create_dataloader(train_data, seq_length, batch_size)\n",
    "val_loader = create_dataloader(val_data, seq_length, batch_size)\n",
    "test_loader = create_dataloader(test_data, seq_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sustainable-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training HyperParameters    \n",
    "num_epochs = 500\n",
    "learning_rate = 0.001\n",
    "print_every = 100\n",
    "\n",
    "input_size = len(features) #number of features\n",
    "hidden_size = 200 #number of features in hidden state\n",
    "num_layers = 2 #number of stacked lstm layers\n",
    "num_classes = 1 #number of output classes \n",
    "fanout_size = 256 # number of fanouts to fully connected layers within the LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chicken-notification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving epoch: 1, Training Loss : 0.02044737454002466814,  Validation Loss: 0.00081080514654466827 ... saving checkpoint 0xxx\n",
      "Saving epoch: 5, Training Loss : 0.00010991716255883275,  Validation Loss: 0.00077121749495130936 ... saving checkpoint 0xxx\n",
      "Saving epoch: 10, Training Loss : 0.00007995735037649527,  Validation Loss: 0.00073243111271310498 ... saving checkpoint 0xxx\n",
      "Saving epoch: 11, Training Loss : 0.00008935371210269338,  Validation Loss: 0.00072863792553795755 ... saving checkpoint 0xxx\n",
      "Saving epoch: 12, Training Loss : 0.00007858694633507340,  Validation Loss: 0.00069916410044439743 ... saving checkpoint 0xxx\n",
      "Saving epoch: 16, Training Loss : 0.00007247450457373273,  Validation Loss: 0.00064745137304369658 ... saving checkpoint 0xxx\n",
      "Saving epoch: 19, Training Loss : 0.00007188589745422269,  Validation Loss: 0.00062652227525423110 ... saving checkpoint 0xxx\n",
      "Saving epoch: 26, Training Loss : 0.00009006791774728589,  Validation Loss: 0.00061196724198305527 ... saving checkpoint 0xxx\n",
      "Saving epoch: 27, Training Loss : 0.00006132700365233458,  Validation Loss: 0.00060164005766194821 ... saving checkpoint 0xxx\n",
      "Saving epoch: 28, Training Loss : 0.00007086634832376149,  Validation Loss: 0.00059125237041027158 ... saving checkpoint 0xxx\n",
      "Saving epoch: 29, Training Loss : 0.00008426414592400944,  Validation Loss: 0.00055446109193301688 ... saving checkpoint 0xxx\n",
      "Saving epoch: 31, Training Loss : 0.00007938361330499987,  Validation Loss: 0.00053505071775509496 ... saving checkpoint 0xxx\n",
      "Saving epoch: 32, Training Loss : 0.00005825475188409423,  Validation Loss: 0.00049620412677098573 ... saving checkpoint 0xxx\n",
      "Saving epoch: 35, Training Loss : 0.00006143353916846789,  Validation Loss: 0.00047177749975408251 ... saving checkpoint 0xxx\n",
      "Saving epoch: 37, Training Loss : 0.00005575615432462655,  Validation Loss: 0.00041130830052409420 ... saving checkpoint 0xxx\n",
      "Saving epoch: 38, Training Loss : 0.00005890914460257429,  Validation Loss: 0.00041022270483694767 ... saving checkpoint 0xxx\n",
      "Saving epoch: 40, Training Loss : 0.00005466739127959320,  Validation Loss: 0.00031061915875840431 ... saving checkpoint 0xxx\n",
      "Saving epoch: 45, Training Loss : 0.00005596720169592999,  Validation Loss: 0.00029325820906779955 ... saving checkpoint 0xxx\n",
      "Saving epoch: 53, Training Loss : 0.00004895742369640633,  Validation Loss: 0.00026895234524901653 ... saving checkpoint 0xxx\n",
      "Saving epoch: 58, Training Loss : 0.00005553937967482669,  Validation Loss: 0.00026677815421371142 ... saving checkpoint 0xxx\n",
      "Saving epoch: 66, Training Loss : 0.00003620794481420059,  Validation Loss: 0.00025009448015496036 ... saving checkpoint 0xxx\n",
      "Saving epoch: 67, Training Loss : 0.00003520649376111204,  Validation Loss: 0.00018129713016851629 ... saving checkpoint 0xxx\n",
      "Saving epoch: 76, Training Loss : 0.00003279746867569214,  Validation Loss: 0.00017131464296284862 ... saving checkpoint 0xxx\n",
      "Saving epoch: 81, Training Loss : 0.00002764849343589577,  Validation Loss: 0.00016956735674505803 ... saving checkpoint 0xxx\n",
      "Saving epoch: 92, Training Loss : 0.00002723994037141399,  Validation Loss: 0.00013869961883363038 ... saving checkpoint 0xxx\n",
      "Saving epoch: 96, Training Loss : 0.00002819582337807763,  Validation Loss: 0.00011556083808426519 ... saving checkpoint 0xxx\n",
      "Saving epoch: 100, Training Loss : 0.00002763382220060281,  Validation Loss: 0.00013924553884961014\n",
      "Saving epoch: 108, Training Loss : 0.00002516572508178543,  Validation Loss: 0.00007299575726564571 ... saving checkpoint 0xxx\n",
      "Saving epoch: 141, Training Loss : 0.00001878869036511950,  Validation Loss: 0.00004809353855750020 ... saving checkpoint 0xxx\n",
      "Saving epoch: 162, Training Loss : 0.00001809889650306901,  Validation Loss: 0.00003848534716117019 ... saving checkpoint 0xxx\n",
      "Saving epoch: 195, Training Loss : 0.00001536688779909078,  Validation Loss: 0.00003639690574444201 ... saving checkpoint 0xxx\n",
      "Saving epoch: 200, Training Loss : 0.00001660886911229527,  Validation Loss: 0.00008252189739298931\n",
      "Saving epoch: 237, Training Loss : 0.00001446929552052337,  Validation Loss: 0.00003607848919125752 ... saving checkpoint 0xxx\n",
      "Saving epoch: 241, Training Loss : 0.00001777422301162823,  Validation Loss: 0.00003603004719283293 ... saving checkpoint 0xxx\n",
      "Saving epoch: 255, Training Loss : 0.00001689435082373039,  Validation Loss: 0.00003255779818180206 ... saving checkpoint 0xxx\n",
      "Saving epoch: 259, Training Loss : 0.00001535284828189684,  Validation Loss: 0.00003234385115541266 ... saving checkpoint 0xxx\n",
      "Saving epoch: 266, Training Loss : 0.00001729576163689467,  Validation Loss: 0.00002918101216818212 ... saving checkpoint 0xxx\n",
      "Saving epoch: 269, Training Loss : 0.00001655461357952937,  Validation Loss: 0.00002880891205112110 ... saving checkpoint 0xxx\n",
      "Saving epoch: 280, Training Loss : 0.00001824771327163091,  Validation Loss: 0.00002863669890978820 ... saving checkpoint 0xxx\n",
      "Saving epoch: 281, Training Loss : 0.00001349421548487347,  Validation Loss: 0.00002692986745878935 ... saving checkpoint 0xxx\n",
      "Saving epoch: 300, Training Loss : 0.00001551878893562473,  Validation Loss: 0.00002930410551630917\n",
      "Saving epoch: 400, Training Loss : 0.00001152426323663581,  Validation Loss: 0.00013451564410462270\n",
      "Saving epoch: 500, Training Loss : 0.00001233364533378730,  Validation Loss: 0.00034789833384943575\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "ckpt_dir = 'stock'\n",
    "min_valid_loss = float('inf')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "    \n",
    "# def __init__(self, num_layers, input_size, hidden_size, fanout_size, output_size, dropout_prob, device):\n",
    "lstm = StockLSTM.StockLSTM(num_layers, input_size, hidden_size, fanout_size, num_classes, 0.25, device) #our lstm class \n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "lstm.to(device)\n",
    "lstm.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for x_tensor, y_tensor in train_loader:\n",
    "        out = lstm.forward(x_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    lstm.eval()\n",
    "    for x_tensor, y_tensor in val_loader:\n",
    "        prediction = lstm.forward(x_tensor)\n",
    "        val_losses.append(criterion(prediction, y_tensor).item())\n",
    "        \n",
    "    # Checkpointing logic\n",
    "    if np.mean(val_losses) < min_valid_loss:\n",
    "        min_valid_loss = np.mean(val_losses)\n",
    "        lstm.save(ckpt_dir + '/lstm_e'+str(int(epoch/1000))+'xxx_lr'+str(learning_rate)+'.pth', np.mean(train_losses), np.mean(val_losses), epoch, learning_rate)\n",
    "        print(\"Saving epoch: %d, Training Loss : %1.20f,  Validation Loss: %1.20f ... saving checkpoint %s\" % (epoch, np.mean(train_losses), np.mean(val_losses), str(int(epoch/1000))+'xxx'))\n",
    "\n",
    "    elif epoch % print_every == 0:\n",
    "        print(\"Saving epoch: %d, Training Loss : %1.20f,  Validation Loss: %1.20f\" % (epoch, np.mean(train_losses), np.mean(val_losses)))\n",
    "        \n",
    "    lstm.train()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "economic-walter",
   "metadata": {},
   "source": [
    "import importlib\n",
    "importlib.reload(StockGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "typical-award",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving epoch: 1, Training Loss : 0.01208971933865118399,  Validation Loss: 0.00052345882176195045 ... saving checkpoint 0xxx\n",
      "Saving epoch: 2, Training Loss : 0.00012951777752059289,  Validation Loss: 0.00051840140666762312 ... saving checkpoint 0xxx\n",
      "Saving epoch: 5, Training Loss : 0.00009010950604935588,  Validation Loss: 0.00051274679434435387 ... saving checkpoint 0xxx\n",
      "Saving epoch: 8, Training Loss : 0.00007743421558061951,  Validation Loss: 0.00045314578282823031 ... saving checkpoint 0xxx\n",
      "Saving epoch: 10, Training Loss : 0.00007251039967868141,  Validation Loss: 0.00043211140311197251 ... saving checkpoint 0xxx\n",
      "Saving epoch: 12, Training Loss : 0.00007123847372126911,  Validation Loss: 0.00042322649765205950 ... saving checkpoint 0xxx\n",
      "Saving epoch: 13, Training Loss : 0.00006681790597043758,  Validation Loss: 0.00042002921376528367 ... saving checkpoint 0xxx\n",
      "Saving epoch: 14, Training Loss : 0.00006503137851970058,  Validation Loss: 0.00037610564440665012 ... saving checkpoint 0xxx\n",
      "Saving epoch: 21, Training Loss : 0.00006349135719926923,  Validation Loss: 0.00035040327300619637 ... saving checkpoint 0xxx\n",
      "Saving epoch: 25, Training Loss : 0.00006799415639888625,  Validation Loss: 0.00026560212309296067 ... saving checkpoint 0xxx\n",
      "Saving epoch: 30, Training Loss : 0.00006966424691143387,  Validation Loss: 0.00024453191184187054 ... saving checkpoint 0xxx\n",
      "Saving epoch: 45, Training Loss : 0.00004442381931196080,  Validation Loss: 0.00022001240073665472 ... saving checkpoint 0xxx\n",
      "Saving epoch: 64, Training Loss : 0.00002744950609829443,  Validation Loss: 0.00018691301001298770 ... saving checkpoint 0xxx\n",
      "Saving epoch: 68, Training Loss : 0.00003389421121126840,  Validation Loss: 0.00016249051102046929 ... saving checkpoint 0xxx\n",
      "Saving epoch: 73, Training Loss : 0.00003660121932653972,  Validation Loss: 0.00014147041912184323 ... saving checkpoint 0xxx\n",
      "Saving epoch: 81, Training Loss : 0.00002342196434213737,  Validation Loss: 0.00012497991367520109 ... saving checkpoint 0xxx\n",
      "Saving epoch: 90, Training Loss : 0.00002257096851584113,  Validation Loss: 0.00011256279764426369 ... saving checkpoint 0xxx\n",
      "Saving epoch: 95, Training Loss : 0.00002480416289416084,  Validation Loss: 0.00009422693118402683 ... saving checkpoint 0xxx\n",
      "Saving epoch: 100, Training Loss : 0.00002785818084470690,  Validation Loss: 0.00011628479842329398\n",
      "Saving epoch: 130, Training Loss : 0.00001893036230546224,  Validation Loss: 0.00009370289331560048 ... saving checkpoint 0xxx\n",
      "Saving epoch: 148, Training Loss : 0.00001932276796129696,  Validation Loss: 0.00009245567284902004 ... saving checkpoint 0xxx\n",
      "Saving epoch: 156, Training Loss : 0.00002091090315540593,  Validation Loss: 0.00006959367464013347 ... saving checkpoint 0xxx\n",
      "Saving epoch: 168, Training Loss : 0.00001641648217515439,  Validation Loss: 0.00006161714348284452 ... saving checkpoint 0xxx\n",
      "Saving epoch: 179, Training Loss : 0.00001654168548248686,  Validation Loss: 0.00006082016615609235 ... saving checkpoint 0xxx\n",
      "Saving epoch: 193, Training Loss : 0.00001828788155988206,  Validation Loss: 0.00006069156645665046 ... saving checkpoint 0xxx\n",
      "Saving epoch: 200, Training Loss : 0.00001670838969579380,  Validation Loss: 0.00006912691743629104\n",
      "Saving epoch: 206, Training Loss : 0.00001765266416237131,  Validation Loss: 0.00005060769772909053 ... saving checkpoint 0xxx\n",
      "Saving epoch: 233, Training Loss : 0.00002021563786135891,  Validation Loss: 0.00004410625361079174 ... saving checkpoint 0xxx\n",
      "Saving epoch: 281, Training Loss : 0.00001337029945448581,  Validation Loss: 0.00004245123017352779 ... saving checkpoint 0xxx\n",
      "Saving epoch: 283, Training Loss : 0.00001612807613508817,  Validation Loss: 0.00003546262873774017 ... saving checkpoint 0xxx\n",
      "Saving epoch: 300, Training Loss : 0.00001558782984879384,  Validation Loss: 0.00003628834113171890\n",
      "Saving epoch: 329, Training Loss : 0.00001508871014090074,  Validation Loss: 0.00003372561961343210 ... saving checkpoint 0xxx\n",
      "Saving epoch: 338, Training Loss : 0.00001236104243927080,  Validation Loss: 0.00003365662686937376 ... saving checkpoint 0xxx\n",
      "Saving epoch: 371, Training Loss : 0.00001452827244474622,  Validation Loss: 0.00003314943018808294 ... saving checkpoint 0xxx\n",
      "Saving epoch: 385, Training Loss : 0.00001310956554334552,  Validation Loss: 0.00003127447576549162 ... saving checkpoint 0xxx\n",
      "Saving epoch: 400, Training Loss : 0.00001297923108546639,  Validation Loss: 0.00004107348074801455\n",
      "Saving epoch: 407, Training Loss : 0.00001247173189795838,  Validation Loss: 0.00003025463752852079 ... saving checkpoint 0xxx\n",
      "Saving epoch: 435, Training Loss : 0.00001245255885953876,  Validation Loss: 0.00002900962703574191 ... saving checkpoint 0xxx\n",
      "Saving epoch: 500, Training Loss : 0.00001077609514394744,  Validation Loss: 0.00004377070271820648\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "ckpt_dir = 'stock'\n",
    "min_valid_loss = float('inf')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "gru = StockGRU.StockGRU(num_layers, input_size, hidden_size, fanout_size, num_classes, 0.25, device) #our gru class \n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)\n",
    "gru.to(device)\n",
    "gru.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for x_tensor, y_tensor in train_loader:\n",
    "        out = gru.forward(x_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    gru.eval()\n",
    "    for x_tensor, y_tensor in val_loader:\n",
    "        prediction = gru.forward(x_tensor)\n",
    "        val_losses.append(criterion(prediction, y_tensor).item())\n",
    "        \n",
    "    # Checkpointing logic\n",
    "    if np.mean(val_losses) < min_valid_loss:\n",
    "        min_valid_loss = np.mean(val_losses)\n",
    "        gru.save(ckpt_dir + '/gru_e'+str(int(epoch/1000))+'xxx_lr'+str(learning_rate)+'.pth', np.mean(train_losses), np.mean(val_losses), epoch, learning_rate)\n",
    "        print(\"Saving epoch: %d, Training Loss : %1.20f,  Validation Loss: %1.20f ... saving checkpoint %s\" % (epoch, np.mean(train_losses), np.mean(val_losses), str(int(epoch/1000))+'xxx'))\n",
    "\n",
    "    elif epoch % print_every == 0:\n",
    "        print(\"Saving epoch: %d, Training Loss : %1.20f,  Validation Loss: %1.20f\" % (epoch, np.mean(train_losses), np.mean(val_losses)))\n",
    "        \n",
    "    gru.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-daniel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
